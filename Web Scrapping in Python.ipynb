{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.danimadrid.net/coding/scraping_youtube_without_api.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\">\n",
      "   <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\">\n",
      "    <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n",
      "     <meta content=\"Coding resources, snippets and samples by a non-digital native\" name=\"description\">\n",
      "      <meta content=\"Dani Madrid-Morales\" name=\"author\">\n",
      "       <title>\n",
      "        Scraping YouTube data without using an API\n",
      "       </title>\n",
      "       <link href=\"../favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\">\n",
      "        <link href=\"../css/bootstrap.min.css\" rel=\"stylesheet\">\n",
      "         <link href=\"../css/clean-blog.min.css\" rel=\"stylesheet\">\n",
      "          <link href=\"../css/prism.css\" rel=\"stylesheet\">\n",
      "           <link href=\"../font-awesome/css/font-awesome.min.css\" rel=\"stylesheet\" type=\"text/css\">\n",
      "            <link href=\"https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic\" rel=\"stylesheet\" type=\"text/css\">\n",
      "             <link href=\"https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800\" rel=\"stylesheet\" type=\"text/css\">\n",
      "              <script src=\"../js/jquery.min.js\">\n",
      "              </script>\n",
      "              <script src=\"../js/bootstrap.min.js\">\n",
      "              </script>\n",
      "              <script src=\"../js/jqBootstrapValidation.js\">\n",
      "              </script>\n",
      "              <script src=\"../js/contact_me.js\">\n",
      "              </script>\n",
      "              <script src=\"../js/clean-blog.min.js\">\n",
      "              </script>\n",
      "              <script src=\"../js/prism.js\">\n",
      "              </script>\n",
      "             </link>\n",
      "            </link>\n",
      "           </link>\n",
      "          </link>\n",
      "         </link>\n",
      "        </link>\n",
      "       </link>\n",
      "      </meta>\n",
      "     </meta>\n",
      "    </meta>\n",
      "   </meta>\n",
      "  </meta>\n",
      " </head>\n",
      " <body>\n",
      "  <nav class=\"navbar navbar-default navbar-custom navbar-fixed-top\">\n",
      "   <div class=\"container-fluid\">\n",
      "    <div class=\"navbar-header page-scroll\">\n",
      "     <button class=\"navbar-toggle\" data-target=\"#bs-example-navbar-collapse-1\" data-toggle=\"collapse\" type=\"button\">\n",
      "      <span class=\"sr-only\">\n",
      "       Toggle navigation\n",
      "      </span>\n",
      "      Menu\n",
      "      <i class=\"fa fa-bars\">\n",
      "      </i>\n",
      "     </button>\n",
      "    </div>\n",
      "    <div class=\"collapse navbar-collapse\" id=\"bs-example-navbar-collapse-1\">\n",
      "     <ul class=\"nav navbar-nav navbar-right\">\n",
      "      <li>\n",
      "       <a href=\"index.html\">\n",
      "        Latests posts\n",
      "       </a>\n",
      "      </li>\n",
      "      <li>\n",
      "       <a href=\"../about/index.html\">\n",
      "        About\n",
      "       </a>\n",
      "      </li>\n",
      "      <li>\n",
      "       <a href=\"../index.html\">\n",
      "        Back to Website\n",
      "       </a>\n",
      "      </li>\n",
      "     </ul>\n",
      "    </div>\n",
      "    <!-- /.navbar-collapse -->\n",
      "   </div>\n",
      "   <!-- /.container -->\n",
      "  </nav>\n",
      "  <header class=\"intro-header\" style=\"background-image: url('img/home-bg.jpg')\">\n",
      "   <div class=\"container\">\n",
      "    <div class=\"row\">\n",
      "     <div class=\"col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1\">\n",
      "      <div class=\"post-heading\">\n",
      "       <h1>\n",
      "        Scraping YouTube data with and without Google's API\n",
      "       </h1>\n",
      "       <h2 class=\"subheading\">\n",
      "        Trying different methods to retrieve metadata and stats of online videos\n",
      "       </h2>\n",
      "       <span class=\"meta\">\n",
      "        Posted by\n",
      "        <a href=\"dani.html\">\n",
      "         Dani Madrid-Morales\n",
      "        </a>\n",
      "        on January 28, 2017\n",
      "       </span>\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </header>\n",
      "  <article>\n",
      "   <div class=\"container\">\n",
      "    <div class=\"row\">\n",
      "     <div class=\"col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1\">\n",
      "      <p>\n",
      "       Lately I have been working on a side project, that looks at one of CGTN-Africa's TV shows,\n",
      "       <em>\n",
      "        Faces of Africa\n",
      "       </em>\n",
      "       and the way it portrays African countries (and Sino-African relations) to investigate whether it does so differently or similarly to US and European mainstream media. Since most of the videos from the show are available on YouTube, I wanted to quickly retrieve \n",
      "                     basic information about each of them (length, date, description...) to get an rough idea of the main themes. I started looking at possible \n",
      "                     ways to extract the data using Google's API and R, and found a neat package called\n",
      "       <a href=\"\" target=\"_blank\">\n",
      "        \"tuber\"\n",
      "       </a>\n",
      "       . \n",
      "                     Tuber comes with the same limitations of the API: the number of queries is limited and you have quotas per query. In most cases \n",
      "                     tuber should be enough to get the information one needs. Tuber offers simple to use functions to retrieve number of times a \n",
      "                     video has been watched, the number of positive and negative votes, the title and description of videos... However, \n",
      "                     there is no function to retrieve more technical data, such as the format an duration of the videos. Since I wanted both \n",
      "                     types of information I decided to test two ways to customize the scraper in order to get all the information that I needed.\n",
      "      </p>\n",
      "      <h2 class=\"section-heading\">\n",
      "       Using YouTube's API\n",
      "      </h2>\n",
      "      <p>\n",
      "       To use tuber, the first thing that is needed is an API OAuth (note that tuber does not use an API key, but requires both a userID and \n",
      "                    a authID) that can be obtained at the Google Developers Console. With this, anybody gets 1 million \"points\" as a daily quota. These are \n",
      "                    enough in most cases. I wanted to get all videos uploaded in a given channel. For this, the process involves three steps:\n",
      "      </p>\n",
      "      <ol>\n",
      "       <li>\n",
      "        Get the ID for the \"uploads\" playlist, which basically means all videos uploaded to a channel\n",
      "       </li>\n",
      "       <li>\n",
      "        Use the ID to generate a list of all the video IDs in the channel\n",
      "       </li>\n",
      "       <li>\n",
      "        Loop through the list to extract the data for each video\n",
      "       </li>\n",
      "      </ol>\n",
      "      <p>\n",
      "       This is what the code looks like using tuber, curl and jsonlite (to convert the data we get without using tuber, as API direct calls \n",
      "                    return JSON files):\n",
      "       <pre><code class=\"language-r\">\n",
      "#Get all the playlists in a channel\n",
      "f = list_channel_resources(filter = c(channel_id = \"UCHBDXQDmqnaqIEPdEapEFVQ\"), part=\"contentDetails\")\n",
      "#Extract the ID of the uploads playlist (i.e. all the videos in a channel)\n",
      "playlist_id = f$items[[1]]$contentDetails$relatedPlaylists$uploads\n",
      "#Loop through the list to grab all IDs in the playlist\n",
      "nextoken = \"\"\n",
      "ids=c()\n",
      "vids = get_playlist_items(filter= c(playlist_id=playlist_id), page_token = nextoken)\n",
      "for(i in 1:as.integer((vids$pageInfo$totalResults/50)+1)){\n",
      "  vids = get_playlist_items(filter= c(playlist_id=playlist_id), page_token = nextoken)\n",
      "  nextoken = vids$nextPageToken\n",
      "  vid_ids = as.vector(unlist(sapply(vids$items, \"[\", \"contentDetails\")))\n",
      "  ids = append(ids, vid_ids)\n",
      "    }\n",
      "#Clean the list, remove the datestamps and keep unique IDs, as soom are repeated\n",
      "allids = ids[c(TRUE, FALSE)]\n",
      "unique(allids)\n",
      "\n",
      "#Build a URL to call the API\n",
      "URL_base='https://www.googleapis.com/youtube/v3/videos?id=' #this is the base URL\n",
      "URL_details='âˆ‚=contentDetails&amp;key;='                     #getting contentDetail for technical metadata\n",
      "URL_key='{Use your own key}'\n",
      "\n",
      "#Loop through URLS to retrieve basic info (duration, format)\n",
      "alldata = data.frame()\n",
      "ptm &lt;- proc.time()                                          #I like to time responses to the server\n",
      "for(i in 1:length(allids)){\n",
      "  url = paste(URL_base, allids[[i]], URL_details, URL_key, sep = \"\")  \n",
      "  result &lt;- fromJSON(txt=url)\n",
      "  id = result$items$id\n",
      "  duration = result$items$contentDetails$duration\n",
      "  caption = result$items$contentDetails$caption\n",
      "  definition = result$items$contentDetails$definition\n",
      "  alldata = rbind(alldata, data.frame(id, duration, caption, definition))\n",
      "  d = get_stats(video_id = allids[[i]])\n",
      "  a = get_video_details(allids[[i]])\n",
      "  e = data.frame(t(do.call(rbind.data.frame, d)))\n",
      "  e$date = a$publishedAt\n",
      "  e$title = a$title\n",
      "  e$describe = a$description\n",
      "  e$source = a$channelTitle\n",
      "  e$sourceID = a$channelId\n",
      "  df = rbind(df, e)\n",
      "  rownames(df) = NULL\n",
      "}\n",
      "  s = merge(df, alldata)\n",
      "proc.time() - ptm\n",
      "                    </code></pre>\n",
      "       <pre><code>\n",
      "           id viewCount likeCount dislikeCount favoriteCount commentCount                     date\n",
      "1 gpa3YN9OC2s       765         6            0             0            0 2017-01-25T08:47:20.000Z\n",
      "2 y0Ev9LBD6Tc       686         6            0             0            0 2017-01-25T08:17:02.000Z\n",
      "3 rog8bizCwvc        99         3            1             0            0 2017-01-25T07:39:41.000Z\n",
      "4 gpa3YN9OC2s       803         6            0             0            0 2017-01-25T08:47:20.000Z\n",
      "5 y0Ev9LBD6Tc       686         6            0             0            0 2017-01-25T08:17:02.000Z\n",
      "6 rog8bizCwvc        99         3            1             0            0 2017-01-25T07:39:41.000Z\n",
      "&gt; \n",
      "                    </code></pre>\n",
      "       <p>\n",
      "        Tuber makes the whole process fairly easy so in most cases, it should be enough to get all the information one needs. There are also some good pieces of Freeware that do the trick, but I wanted to write my own script that would bypass the API to get the data. Scraping YouTube directly is not the fastest way to get data, but it gets the job done fairly simply. For my research I deal with channels that have thousands and tens of thousands of videos. For these big projects, quotas run out fairly quickly.\n",
      "       </p>\n",
      "       <h2 class=\"section-heading\">\n",
      "        Scraping YouTube without the API\n",
      "       </h2>\n",
      "       <p>\n",
      "        I built a very simple script around\n",
      "        <a href=\"\" target=\"_blank\">\n",
      "         rvest\n",
      "        </a>\n",
      "        to scrape the elements and\n",
      "        <a href=\"\" target=\"_blank\">\n",
      "         xml2\n",
      "        </a>\n",
      "        to read the html file. It simply loops over a list of links and extracts the values from either the metadata or the body of the page and then stores them into a df structure. It is nothing fancy but it does the trick, particularly if we already have a previous list with all the URLs we want to loop through.\n",
      "       </p>\n",
      "       <pre><code class=\"language-r\">\n",
      "#Read a single URL\n",
      "youtube_url = read_html(\"https://www.youtube.com/watch?v=sb-NRYmm79g\")\n",
      "## Manually build a list of videos\n",
      "youtube_list = data.frame(url=c(\"https://www.youtube.com/watch?v=sb-NRYmm79g\",\n",
      "                            \"https://www.youtube.com/watch?v=EA9_oNGsW9k\",\n",
      "                            \"https://www.youtube.com/watch?v=S6m8oSYjvfs\",\n",
      "                            \"https://www.youtube.com/watch?v=Mkn9AbISAb8\"))\n",
      "#Read a list of links from a CSV file with a column named url with all the URLS to mine\n",
      "youtube_list = read.csv(\"FoA_Links.csv\", header = TRUE, sep = \";\")\n",
      "\n",
      "#Setting up empty df to store data\n",
      "    temp.df = data.frame(id=\"\", date=\"\", title=\"\", duration=\"\", mins=\"\", secs=\"\",\n",
      "                         description=\"\", views= \"\", pos=\"\", neg=\"\", fullurl=\"\")\n",
      "    youtube.df = data.frame() #Will include the final outcome\n",
      "\n",
      "#Loop through the list of links and extract some general metadata\n",
      "for(i in 1:length(youtube_list$url)){\n",
      "    youtube_url = read_html(as.character(youtube_list$url[[i]]))\n",
      "    id = as.character(html_nodes(youtube_url, 'meta[itemprop=\"videoId\"]') %&gt;% \n",
      "                    html_attr(\"content\"))\n",
      "    date = as.character(html_nodes(youtube_url, 'meta[itemprop=\"datePublished\"]') %&gt;% \n",
      "                    html_attr(\"content\"))\n",
      "    title = as.character(html_nodes(youtube_url, 'meta[itemprop=\"name\"]') %&gt;% \n",
      "                    html_attr(\"content\"))\n",
      "    mins = as.numeric(gsub(\"M\",\"\",str_extract(as.character(html_nodes(youtube_url, 'meta[itemprop=\"duration\"]') %&gt;% \n",
      "                    html_attr(\"content\")), \"\\\\d*M\")))\n",
      "    secs = as.numeric(gsub(\"S\",\"\",str_extract(as.character(html_nodes(youtube_url, 'meta[itemprop=\"duration\"]') %&gt;% \n",
      "                    html_attr(\"content\")), \"\\\\d*S\")))\n",
      "    duration = (mins*60) + secs\n",
      "    description = as.character(html_node(youtube_url, '#eow-description') %&gt;% \n",
      "                                  html_text())\n",
      "    views = as.numeric(html_nodes(youtube_url, 'meta[itemprop=\"interactionCount\"]') %&gt;% \n",
      "                    html_attr(\"content\"))  \n",
      "    try({\n",
      "      pos = html_nodes(youtube_url, 'span.yt-uix-button-content') %&gt;% \n",
      "        html_text()\n",
      "      pos = as.numeric(gsub(\",\", \"\", pos[15]))}, silent = TRUE)\n",
      "    if(length(pos)==0){\n",
      "      pos=NA\n",
      "    }\n",
      "    try({\n",
      "      neg = html_nodes(youtube_url, 'span.yt-uix-button-content') %&gt;% \n",
      "        html_text()\n",
      "      neg = as.numeric(gsub(\",\", \"\", neg[18]))}, silent = TRUE)\n",
      "    if(length(neg)==0){\n",
      "      neg=NA\n",
      "    }\n",
      "    fullurl = paste(\"https://www.youtube.com/watch?v=\",id, sep=\"\")\n",
      "#Saves output into a df and appends the data to the final df\n",
      "    temp.df = data.frame(id, date, title, duration, description, mins, secs, views, pos, neg, fullurl)\n",
      "    youtube.df = rbind(youtube.df, temp.df)\n",
      "\n",
      "#Empties all the fields before creating a new entry \n",
      "    temp.df = data.frame(id=\"\", date=\"\", title=\"\", duration=\"\", mins=\"\", secs=\"\", description=\"\",  \n",
      "                        views= \"\", pos=\"\", neg=\"\", fullurl=\"\")\n",
      "#Clear all temp variables    \n",
      "    remove(id, date, title, duration, description,views, pos, neg, fullurl, mins, secs)\n",
      "}\n",
      "#Delete temporary df\n",
      "    remove(temp.df, youtube_url, i, youtube_list)\n",
      "                    </code></pre>\n",
      "       <p>\n",
      "       </p>\n",
      "      </p>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </article>\n",
      "  <hr>\n",
      "   <footer>\n",
      "    <div class=\"container\">\n",
      "     <div class=\"row\">\n",
      "      <div class=\"col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1\">\n",
      "       <ul class=\"list-inline text-center\">\n",
      "        <li>\n",
      "         <a href=\"https://twitter.com/DMadrid_M/\" target=\"_blank\">\n",
      "          <span class=\"fa-stack fa-lg\">\n",
      "           <i class=\"fa fa-circle fa-stack-2x\">\n",
      "           </i>\n",
      "           <i class=\"fa fa-twitter fa-stack-1x fa-inverse\">\n",
      "           </i>\n",
      "          </span>\n",
      "         </a>\n",
      "        </li>\n",
      "        <li>\n",
      "         <a href=\"https://hk.linkedin.com/in/danimadridmorales\" target=\"_blank\">\n",
      "          <span class=\"fa-stack fa-lg\">\n",
      "           <i class=\"fa fa-circle fa-stack-2x\">\n",
      "           </i>\n",
      "           <i class=\"fa fa-linkedin fa-stack-1x fa-inverse\">\n",
      "           </i>\n",
      "          </span>\n",
      "         </a>\n",
      "        </li>\n",
      "        <li>\n",
      "         <a href=\"https://scholar.google.com/citations?user=wHTojqYAAAAJ&amp;hl=en\" target=\"_blank\">\n",
      "          <span class=\"fa-stack fa-lg\">\n",
      "           <i class=\"fa fa-circle fa-stack-2x\">\n",
      "           </i>\n",
      "           <i class=\"fa fa-google fa-stack-1x fa-inverse\">\n",
      "           </i>\n",
      "          </span>\n",
      "         </a>\n",
      "        </li>\n",
      "       </ul>\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "   </footer>\n",
      "  </hr>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "urllib_page = urllib.request.urlopen(url)\n",
    "soup_urllib= bs(urllib_page,'html.parser')\n",
    "print(soup_urllib.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Scraping YouTube data without using an API</title>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_urllib.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraping YouTube data without using an API'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_urllib.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"index.html\">Latests posts</a>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_urllib.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index.html\n",
      "../about/index.html\n",
      "../index.html\n",
      "dani.html\n",
      "\n",
      "\n",
      "\n",
      "https://twitter.com/DMadrid_M/\n",
      "https://hk.linkedin.com/in/danimadridmorales\n",
      "https://scholar.google.com/citations?user=wHTojqYAAAAJ&hl=en\n"
     ]
    }
   ],
   "source": [
    "all_links=soup_urllib.find_all('a')\n",
    "for link in all_links:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tables=soup_urllib.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
